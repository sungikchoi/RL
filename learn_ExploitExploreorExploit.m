function[pi,n_trials]=learn_ExploitExploreorExploit(init_state, n_states, n_actions, n_episodes, alpha, gamma, reward, terminal, new_state)
%Initialization: Initially set S of known states is empty
%(Balanced wandering) Any time the current state is not in S, the algorithm
%performs balanced wandering
%(Discovery of New known states): Any time a state i has been visited
%mknown times during balanced wnadering, it enters the known set S and no
%longer participates in balanced wandering
%(Off-line Optimizations) Upon reaching a known state i\in S during
%balanced wandering, the algorithm performs the two off-line policy
%computations on Ms and Ms(prime) 
%If the resulting exploitation policy pihat achieves retrun from i in Mshat
%that is at least V*-epsilon/2, the algorithm executes for the next T steps
%Otherwise, the algorithm executes the resulting exploration policy
%pi(prime) for T steps in M
%Any time an attempted exploitation or attempted exploration visits a state
%not in S, the algorithm immediately resumes balanced wandering.
epsilon=0.1;
delta=0.001;
horizontime=ceil(1/(1-gamma)*log(max(max(reward))/epsilon/(1-gamma)));
actstate=reward;
sumstate=zeros(n_states,1);
for i=1:n_states%Calculate the variance
    if(terminal(i)==0)
        for j=1:n_actions
            if(new_state(i,j)~=0)
                sumstate(i)=sumstate(i)+1;
            end
        end
    end
end
meanstate=zeros(n_states,1);
for i=1:n_states
    meanstate(i)=sum(actstate(i,:))/sumstate(i);
end
varstate=zeros(n_states,1);
for i=1:n_states
    if(terminal(i)==0)
        for j=1:n_actions
            if(new_state(i,j)~=0)
                varstate(i)=varstate(i)+(actstate(i,j)-meanstate(i))^2/sumstate(i);
            end
        end
    end
end
varmax=max(varstate);
Gmax=1;
mcal=ceil((n_states*horizontime*Gmax)^4*varmax*log(1/delta));
mcal=2;
%horizontime=40;
memory=zeros(n_states,n_actions);
insideS=zeros(n_states,1);
visited=zeros(n_states,1);
n_trials=zeros(n_episodes,1);
t=0;
maxreward=max(max(reward));
newstatepihat=zeros(n_states+1,n_actions);
for i=1:n_actions
    newstatepihat(n_states+1,i)=n_states+1;
end
rewardpihat=zeros(n_states+1,1);
rewardpiprime=zeros(n_states+1,1);
for i=1:n_actions
    rewardpiprime(n_states+1,1)=maxreward;
end

for k=1:n_episodes
    s=init_state;
    visited(s)=visited(s)+1;
    Itercount=0;%This is for the sign of policy exploration//exploitation
    for j=1:1e9
        t=t+1;
        if(insideS(s)==0)%If not known state, do balanced wandering
            Itercount=0;
            visitedmin=mcal+1;
            for i=1:n_actions %Find action with leas visited actions
                if(new_state(s,i)~=0)
                    if(visitedmin>memory(s,i))
                        actionmin=i;
                        visitedmin=memory(s,i);
                    end
                end
            end
            a=actionmin;%Choose the lowest selected actions
        end
        if(insideS(s)==1&&Itercount==0)%If known state, do explore or exploit
            if(rand(1)<40/t) %Do exploration
                [pi]=valiterE3(n_states+1,n_actions,horizontime,rewardpiprime,newstatepihat,gamma); 
            else%Do exploitation
                [pi]=valiterE3(n_states+1,n_actions,horizontime,rewardpihat,newstatepihat,gamma);
            end
            Itercount=horizontime;
            a=pi(horizontime-Itercount+1,s);
            Itercount=Itercount-1;
        end
        if(insideS(s)==1&&Itercount>0)%Known state, and we need more exploration-> Do piprime
            a=pi(horizontime-Itercount+1,s);
            Itercount=Itercount-1;
        end
        sn=new_state(s,a);
        memory(s,a)=memory(s,a)+1;
        visited(sn)=visited(sn)+1;
        for statecheck=1:n_states
            if(visited(statecheck)>=mcal) %Make the state visited a lot as the known state
                insideS(statecheck)=1;
                for actioncheck=1:n_actions
                    resultstate=new_state(statecheck,actioncheck);
                    if resultstate~=0
                        if(visited(resultstate)>mcal)
                            newstatepihat(statecheck,actioncheck)=resultstate;
                            rewardpihat(resultstate)=reward(statecheck,actioncheck);
                            rewardpiprime(resultstate)=0;
                        else
                            newstatepihat(statecheck,actioncheck)=n_states+1;
                            rewardpihat(resultstate)=0;
                            rewardpiprime(resultstate)=maxreward;
                        end
                    end
                end
            end   
        end
        if terminal(sn)
            n_trials(k)=j;
            break;
        end
        s=sn;
    end
end
end